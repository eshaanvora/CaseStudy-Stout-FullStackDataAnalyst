---
title: "Case Study 1"
author: "Eshaan Vora"
subtitle: "Stout: Full Stack Data Analyst"
output: 
  html_document:
  df_print: paged
html_notebook: default
---
  
```{r setup, include=FALSE}

library('dplyr')
library('glmnet')
library('glmnetUtils')
library('randomForest')

library("readr")
library("tidyverse")
library("rsample")
#library('forcats')
library('coefplot')
library('data.table')
library('ggridges')
library('gganimate')
library('tidyr')
library("magrittr")
#library("sjPlot") //tab_model()
library('ggplot2')
library('yardstick')
library('plotROC')

#Define functions
#Function to count missing observationsper every variable (column) in the data
num_missing_val <- function(data_frame){
  for(i in 1:ncol(data_frame)){
    print(paste0("Variable: ", names(data_frame)[i], " NA Count: ", sum(is.na(data_frame %>% select(i)))))
  }
}

```

## Load & Clean Data
```{r}

#Update file path
filePath = "/Users/eshaan/Downloads/CaseStudy_Stout/loans_full_schema.csv"
data = read.csv(filePath, stringsAsFactors = TRUE)

#Check for missing values
#num_missing_val(data_clean)

#Filter data based on whether the filer is a single filer
data_clean <- subset(data, is.na(annual_income_joint))

data_clean <- select(data_clean,-c(emp_title, emp_length, application_type, annual_income_joint, debt_to_income_joint, verification_income_joint, current_accounts_delinq, num_accounts_120d_past_due, num_accounts_30d_past_due))

#Change "N/A" value to the largest amount of months since delinquency
#This is assuming filers with an "N/A" for the variable "months since delinquency" have never been delinquent and so they should be imputed with the largest value
data_clean$months_since_last_delinq <- data_clean$months_since_last_delinq %>% replace_na(max(data_clean$months_since_last_delinq, na.rm=T))

data_clean$months_since_90d_late <- data_clean$months_since_90d_late %>% replace_na(max(data_clean$months_since_90d_late, na.rm=T))

data_clean$months_since_last_credit_inquiry <- data_clean$months_since_last_credit_inquiry %>% replace_na(max(data_clean$months_since_last_credit_inquiry, na.rm=T))

#data_clean$num_accounts_120d_past_due <- data_clean$num_accounts_120d_past_due %>% replace_na(max(data_clean$num_accounts_120d_past_due, na.rm=T))


```

## Split Data into Training and Test Data
```{r}

set.seed(1999)
split_data = sort(sample(nrow(data_clean), nrow(data_clean)*.8))
train<-data_clean[split_data,]
test<-data_clean[-split_data,]

```

## Model Prediction
```{r}
#LASSO REGRESSION MODEL
lasso_model <- cv.glmnet(interest_rate ~ . - grade, data = train, alpha = 1)

plot(lasso_model)

results_lasso <- cbind(predict(lasso_model, test), test$interest_rate)

#OLS REGRESSION MODEL
#Determine statistical significance of each variable and of each factor level within string variables
#Discard insignificant variables from future models
linear_model <- lm(interest_rate ~ . - grade, data = train)
summary(linear_model)

#Determine Variable Importance to discard unimportant variables
#Determine which variables affect prediction the most at lambda.1se (or 1 standard error away from prediction)
#Reference:https://localcoder.org/glmnet-variable-importance
coefList <- coef(lasso_model, s='lambda.1se')
coefList <- data.frame(coefList@Dimnames[[1]][coefList@i+1],coefList@x)
names(coefList) <- c('Variable','Coefficient')


#RANDOM FOREST MODEL
#Error begins to plateau when we use 150 decision trees
random_forest_model <- randomForest(interest_rate ~ . - grade -loan_status -term -total_debit_limit, data = data_clean, mtry = 5, importance = TRUE, ntree=150)

print(random_forest_model)

results_random_forest <- data.frame(predict(random_forest_model, test), test$interest_rate)

plot(random_forest_model)

```

## Data Visualizations
```{r}

#The lower-grade the investment, the riskier and therefore the higher the interest rates
group_by_grade <- data_clean %>% group_by(grade, sub_grade) %>% summarise(meanInterestRate = mean(interest_rate)) %>% rename(Investment_Grade = grade)

ggplot(group_by_grade, aes(y = meanInterestRate, x=Investment_Grade, color=sub_grade)) + 
  geom_bar(position="dodge", stat="identity")

##Interest Rate by State (Are certain state potentially more expensive to borrow in?)
group_by_state <- data_clean %>% group_by(state) %>% summarise(meanInterestRate = mean(interest_rate))

ggplot(group_by_state, aes(y = meanInterestRate, x=state, fill=state)) + 
  geom_bar(position="dodge", stat="identity")

#Are certain debt purchases riskier to lend to than others? (Therefore commanding a higher interest rate)
group_by_loan <- data_clean %>% group_by(loan_purpose) %>% summarise(meanInterestRate = mean(interest_rate), meanCreditUse = mean(total_credit_utilized))

ggplot(group_by_loan, aes(y = meanInterestRate, x=loan_purpose, fill=loan_purpose)) + 
  geom_bar(position="dodge", stat="identity")

#Delinquency
ggplot(data_clean, aes(x = interest_rate, y=account_never_delinq_percent)) + geom_bin2d() + ggtitle("Percent of Credit Lines that Never Went Delinquent VS. Effective Interest Rate") +
  xlab("Interest Rate") + ylab("Percent of Credit Lines not Delinquent")


#Debit Accounts and Debt to Income
ggplot(data_clean, aes(x = num_active_debit_accounts, y=debt_to_income)) + geom_bin2d() + ggtitle("Number of Active Debit Accounts VS. Debt-to-Income") +
  xlab("Active Debit Accounts") + ylab("Debt-to-Income Ratio")

#Employement Type
ggplot(group_by_loan, aes(y = meanCreditUse, x=loan_purpose, color = loan_purpose)) + 
  geom_bar(position="dodge", stat="identity") + ggtitle("Purpose of Loan VS. Credit Utilization") +
  xlab("Purpose of Loan") + ylab("Credit Usage ($)")
```
